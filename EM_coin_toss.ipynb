{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we are given two coins A and B with unkown bias $\\theta_A$ and $\\theta_B$, respectively. Our goal is to estimate the bias vector $\\boldsymbol{\\theta}= (\\theta_A, \\theta_B)$ from the outcomes of the following experiment: \n",
    "\n",
    "<blockquote> \n",
    "First choose one coin at random. Then toss the selected coin 10 times independently and record the number of heads observed. Repeat this procedure 5 times.\n",
    "</blockquote>\n",
    "\n",
    "Formally, let $z_i\\in\\{A,B\\}$ be the coin selected in experiment $i$ and $x_i\\in\\{0,1,\\cdots 10\\}$ be the number heads recorded by tossing $z_i$ 10 times. Since we conduct $n=5$ such experiments, we can summarize the outcomes of these 50 tosses by two vectors: $\\boldsymbol{x}=(x_1,x_2\\cdots, x_5)$ and $\\boldsymbol{z}=(z_1,z_2,\\cdots, z_5)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we know everything?\n",
    "\n",
    " * Consider first the case where we have complete knowledge of the experiment, namely, both $\\boldsymbol{x}$ and $\\boldsymbol{z}$ are known. How would you intuitively estimate the biases of the two coins  $\\boldsymbol{\\theta}= (\\theta_A, \\theta_B)$ ?\n",
    " \n",
    " * What's the likelihood of observing the complete outcomes of these experiments? In other words, what is $P(\\boldsymbol{x},\\boldsymbol{z}| n,\\boldsymbol{\\theta} )$? You may assume this is a Bernoulli trial. Namely, every time coin A(B) is tossed, we have, with probability $\\theta_A$($\\theta_B$), that the outcome is heads.\n",
    " \n",
    " * What's the Maximum Likelihood Estimator (MLE)? Is this consistent with your intuition? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing MLE and EM\n",
    "\n",
    "To test your answer, let's do some numerics! We will compare the MLE estimates of biases with an Expectation Maximization procedure where we do not know ${\\bf z}$. The following code computes our best guess for the biases using MLE -- assuming we know the identity of the coin used -- and compares it estimates arrived at using an EM procedure where we have no knowledge about which coin was being tossed (though we know the same coin was tossed 10 times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import comb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_coin_tosses = 10\n",
    "n_expts = 5\n",
    "thetaA_heads = 0.8\n",
    "thetaB_heads = 0.4\n",
    "\n",
    "coin_choice = np.zeros(n_expts)\n",
    "head_counts = np.zeros(n_expts)\n",
    "    \n",
    "MLE_A = 0.0\n",
    "MLE_B = 0.0\n",
    "\n",
    "for expt in range(n_expts):\n",
    "    if np.random.randint(2) == 0: ## coin A is selected\n",
    "        coin_choice[expt] = 0\n",
    "        head_counts[expt] = np.random.binomial(n_coin_tosses, thetaA_heads, 1)\n",
    "        MLE_A += head_counts[expt]\n",
    "    else: ## coin B is selected\n",
    "        coin_choice[expt] = 1\n",
    "        head_counts[expt] = np.random.binomial(n_coin_tosses, thetaB_heads, 1)\n",
    "        MLE_B += head_counts[expt]\n",
    "    \n",
    "MLE_A = MLE_A / ((n_expts - np.count_nonzero(coin_choice))*n_coin_tosses)\n",
    "MLE_B = MLE_B / (np.count_nonzero(coin_choice)*n_coin_tosses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expectation Maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will come in use below to compute the likelihood for the random binomial distribution\n",
    "\n",
    "def compute_likelihood(n, k, pheads): \n",
    "\n",
    "    likelihood = comb(n, k, exact=True)*(pheads**k)*(1.0-pheads)**(n-k)\n",
    "\n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 0: thetaA = 0.836770,  thetaB = 0.651211\n",
      "At iteration 1: thetaA = 0.901389,  thetaB = 0.604021\n",
      "At iteration 2: thetaA = 0.919082,  thetaB = 0.573289\n",
      "At iteration 3: thetaA = 0.922492,  thetaB = 0.564862\n",
      "At iteration 4: thetaA = 0.923004,  thetaB = 0.562855\n",
      "At iteration 5: thetaA = 0.923036,  thetaB = 0.562315\n",
      "\n",
      "E-M converges at iteration 6\n",
      "RESULT of E-M:\n",
      "theta_A = 0.923036,  theta_B = 0.562315\n",
      "MLE with complete data: theta_A = 0.933333,  theta_B = 0.550000\n"
     ]
    }
   ],
   "source": [
    "# Initialize the coin biases pA_heads = pA(heads), pB_heads = pB(heads)\n",
    "pA_heads = []\n",
    "pB_heads = []\n",
    "\n",
    "# random initialization\n",
    "pA_heads.append(0.60)\n",
    "pB_heads.append(0.50)\n",
    "\n",
    "# EM begins now\n",
    "epsilon = 0.001 # error threshold\n",
    "j = 0  #improvement/epoch counter\n",
    "improvement = float('inf')\n",
    "\n",
    "while improvement > epsilon:\n",
    "    expectation_A = np.zeros((n_expts, 2), dtype=float)\n",
    "    expectation_B = np.zeros((n_expts, 2), dtype=float)\n",
    "    \n",
    "    for expt in range(n_expts):\n",
    "        eH = head_counts[expt]\n",
    "        eT = n_coin_tosses - head_counts[expt]\n",
    "        \n",
    "        # Expectation Step\n",
    "        lA = compute_likelihood(n_coin_tosses, eH, pA_heads[j])\n",
    "        lB = compute_likelihood(n_coin_tosses, eH, pB_heads[j])\n",
    "        \n",
    "        weightA = lA / (lA + lB)\n",
    "        weightB = lB / (lA + lB)\n",
    "        \n",
    "        expectation_A[expt] = np.multiply(weightA, [eH, eT])\n",
    "        expectation_B[expt] = np.multiply(weightB, [eH, eT])\n",
    "    \n",
    "    # Maximization Step\n",
    "    thetaA = np.sum(expectation_A, axis=0)[0] / np.sum(expectation_A)\n",
    "    thetaB = np.sum(expectation_B, axis=0)[0] / np.sum(expectation_B)\n",
    "    print(\"At iteration %d: thetaA = %2f,  thetaB = %2f\" % (j, thetaA, thetaB))\n",
    "    \n",
    "    pA_heads.append(thetaA)\n",
    "    pB_heads.append(thetaB)\n",
    "    \n",
    "    improvement = max(abs(np.array([pA_heads[j+1], pB_heads[j+1]]) - np.array([pA_heads[j], pB_heads[j]])))\n",
    "    \n",
    "    j += 1\n",
    "    \n",
    "# END of E-M, print the outcome\n",
    "print(\"\")\n",
    "print(\"E-M converges at iteration %d\" %j)\n",
    "print(\"RESULT of E-M:\")\n",
    "print(\"theta_A = %2f,  theta_B = %2f\" % (thetaA, thetaB))\n",
    "print(\"MLE with complete data: theta_A = %2f,  theta_B = %2f\" % (MLE_A, MLE_B))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
